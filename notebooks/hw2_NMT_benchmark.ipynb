{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NMT Homework (Self-Contained): EN→DE\n",
        "\n",
        "Train a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n",
        "\n",
        "Focus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n",
        "\n",
        "Data: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "Use `install.sh` or `pip install -r requirements.txt` to set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "import torch, sys, os, math, random\n",
        "print('PyTorch version:', torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "try: sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception: pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Shared Utilities (no external imports)\n",
        "Tokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "SPECIAL_TOKENS = {'pad': '<pad>', 'sos': '<sos>', 'eos': '<eos>', 'unk': '<unk>'}\n",
        "\n",
        "def simple_tokenize(s: str) -> List[str]:\n",
        "    return s.strip().lower().split()\n",
        "\n",
        "def read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n",
        "    pairs: List[Tuple[List[str], List[str]]] = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip('\n",
        "').split('\t')\n",
        "            if len(parts) < 2: continue\n",
        "            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n",
        "    return pairs\n",
        "\n",
        "def build_vocab(seqs: Iterable[List[str]], max_size: int | None = None) -> Dict[str, int]:\n",
        "    from collections import Counter\n",
        "    c = Counter();\n",
        "    for s in seqs: c.update(s)\n",
        "    itms = c.most_common(max_size) if max_size else c.items()\n",
        "    stoi = {SPECIAL_TOKENS['pad']:0, SPECIAL_TOKENS['sos']:1, SPECIAL_TOKENS['eos']:2, SPECIAL_TOKENS['unk']:3}\n",
        "    for w,_ in itms:\n",
        "        if w not in stoi: stoi[w] = len(stoi)\n",
        "    return stoi\n",
        "\n",
        "def encode(tokens: List[str], stoi: Dict[str,int], add_sos_eos: bool=False) -> List[int]:\n",
        "    ids = [stoi.get(t, stoi[SPECIAL_TOKENS['unk']]) for t in tokens]\n",
        "    if add_sos_eos: ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']] ]\n",
        "    return ids\n",
        "\n",
        "class Example:\n",
        "    def __init__(self, s: List[int], ti: List[int], to: List[int]): self.src_ids=s; self.tgt_in_ids=ti; self.tgt_out_ids=to\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
        "        self.examples: List[Example] = []\n",
        "        for src, tgt in pairs:\n",
        "            s = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
        "            t = encode(tgt, tgt_stoi, add_sos_eos=True)\n",
        "            self.examples.append(Example(s, t[:-1], t[1:]))\n",
        "    def __len__(self): return len(self.examples)\n",
        "    def __getitem__(self, i): return self.examples[i]\n",
        "\n",
        "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
        "    src_max = max(len(x.src_ids) for x in batch); tgt_max = max(len(x.tgt_in_ids) for x in batch)\n",
        "    def pad_to(a, L, pad): return a + [pad]*(L-len(a))\n",
        "    src    = torch.tensor([pad_to(x.src_ids,    src_max, pad_id_src) for x in batch])\n",
        "    tgt_in = torch.tensor([pad_to(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    tgt_out= torch.tensor([pad_to(x.tgt_out_ids,tgt_max, pad_id_tgt) for x in batch])\n",
        "    src_l  = torch.tensor([len(x.src_ids)    for x in batch])\n",
        "    tgt_l  = torch.tensor([len(x.tgt_out_ids)for x in batch])\n",
        "    return src, src_l, tgt_in, tgt_out, tgt_l\n",
        "\n",
        "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
        "    if token_count==0: return float('inf')\n",
        "    try: return float(math.exp(loss_sum/token_count))\n",
        "    except OverflowError: return float('inf')\n",
        "\n",
        "def corpus_bleu(refs: List[List[str]], hyps: List[List[str]], max_order: int=4, smooth: bool=True) -> float:\n",
        "    from collections import Counter\n",
        "    def ngrams(t,n): return Counter([tuple(t[i:i+n]) for i in range(len(t)-n+1)])\n",
        "    m=[0]*max_order; p=[0]*max_order; rl=0; hl=0\n",
        "    for r,h in zip(refs,hyps): rl+=len(r); hl+=len(h);\n",
        "        for n in range(1,max_order+1):\n",
        "            R=ngrams(r,n); H=ngrams(h,n);\n",
        "            m[n-1]+=sum(min(c,H[g]) for g,c in R.items()); p[n-1]+=max(len(h)-n+1,0)\n",
        "    prec=[(m[i]+1)/(p[i]+1) if smooth else (m[i]/p[i] if p[i]>0 else 0.0) for i in range(max_order)]\n",
        "    geo=math.exp(sum((1/max_order)*math.log(x) for x in prec if x>0)) if min(prec)>0 else 0.0\n",
        "    bp=1.0 if hl>rl else math.exp(1-rl/max(1,hl))\n",
        "    return float(geo*bp)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "    model.eval(); tot=0.0; toks=0\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        tgt_in,tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
        "        logits = model(src, src_l, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
        "    return tot, toks\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bleu(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int=100):\n",
        "    model.eval(); refs=[]; hyps=[]\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        pred = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        for b in range(src.size(0)):\n",
        "            ref_ids = tgt_out[b].tolist(); hyp_ids = pred[b].tolist()\n",
        "            if eos_id in ref_ids: ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
        "            if eos_id in hyp_ids: hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
        "            refs.append([tgt_itos[i] for i in ref_ids if i!=0])\n",
        "            hyps.append([tgt_itos[i] for i in hyp_ids if i!=0 and i!=sos_id])\n",
        "    return float(corpus_bleu(refs, hyps))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Paths and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "train_path = 'dataset_splits/train.txt'\n",
        "val_path   = 'dataset_splits/val.txt'\n",
        "public_test_path = 'dataset_splits/public_test.txt'\n",
        "if not os.path.exists(public_test_path):\n",
        "    alt = 'dataset_splits/test_public.txt'\n",
        "    public_test_path = alt if os.path.exists(alt) else public_test_path\n",
        "private_test_path = 'dataset_splits/private_test.txt'\n",
        "src_vocab_size = 30000; tgt_vocab_size = 30000\n",
        "emb_dim = 256; hid_dim = 512; layers = 1; dropout = 0.1\n",
        "batch_size = 64; epochs = 5; lr = 3e-4; max_decode_len = 100\n",
        "save_dir = 'checkpoints'; os.makedirs(save_dir, exist_ok=True)\n",
        "print('Public test path:', public_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Loading splits...')\n",
        "train_pairs = read_split(train_path); val_pairs = read_split(val_path); test_pairs = read_split(public_test_path)\n",
        "print(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\n",
        "src_stoi = build_vocab((s for s,_ in train_pairs), max_size=src_vocab_size)\n",
        "tgt_stoi = build_vocab((t for _,t in train_pairs), max_size=tgt_vocab_size)\n",
        "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]; pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
        "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]; eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
        "train_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi); val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi); test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\n",
        "collate = lambda b: collate_pad(b, pad_id_src, pad_id_tgt)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "tgt_itos = [None]*len(tgt_stoi);\n",
        "for w,i in tgt_stoi.items():\n",
        "    if 0<=i<len(tgt_itos): tgt_itos[i]=w\n",
        "print('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Model (Your Playground)\n",
        "Keep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__(); self.emb=nn.Embedding(vocab_size, emb_dim, padding_idx=0);\n",
        "        self.rnn=nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
        "    def forward(self, src, src_lens):\n",
        "        emb=self.emb(src); packed=nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out,(h,c)=self.rnn(packed); out,_=nn.utils.rnn.pad_packed_sequence(out, batch_first=True); return out,(h,c)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__(); self.emb=nn.Embedding(vocab_size, emb_dim, padding_idx=0);\n",
        "        self.rnn=nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0); self.proj=nn.Linear(hid_dim, vocab_size)\n",
        "    def forward(self, tgt_in, hidden):\n",
        "        emb=self.emb(tgt_in); out,hidden=self.rnn(emb, hidden); return self.proj(out), hidden\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc, dec): super().__init__(); self.encoder=enc; self.decoder=dec\n",
        "    def forward(self, src, src_lens, tgt_in):\n",
        "        _,h=self.encoder(src, src_lens); logits,_=self.decoder(tgt_in, h); return logits\n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "        B=src.size(0); _,h=self.encoder(src, src_lens); inputs=torch.full((B,1), sos_id, dtype=torch.long, device=src.device); outs=[]\n",
        "        for _ in range(max_len):\n",
        "            logits,h=self.decoder(inputs[:,-1:].contiguous(), h); nxt=logits[:,-1,:].argmax(-1, keepdim=True); outs.append(nxt); inputs=torch.cat([inputs,nxt], dim=1)\n",
        "        seqs=torch.cat(outs, dim=1);\n",
        "        for i in range(B):\n",
        "            row=seqs[i];\n",
        "            if (row==eos_id).any(): idx=(row==eos_id).nonzero(as_tuple=False)[0].item(); row[idx+1:]=eos_id\n",
        "        return seqs\n",
        "encoder=Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout); decoder=Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "model=Seq2Seq(encoder, decoder).to(device); optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train(); tot=0.0; toks=0\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in train_loader:\n",
        "        src,src_l=src.to(device), src_l.to(device); tgt_in,tgt_out=tgt_in.to(device), tgt_out.to(device)\n",
        "        optimizer.zero_grad(); logits=model(src, src_l, tgt_in)\n",
        "        loss=criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)); loss.backward();\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
        "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
        "    tr_ppl=compute_perplexity(tot,toks); v_loss,v_toks=evaluate_nll(val_loader, model, pad_id_tgt, device); v_ppl=compute_perplexity(v_loss,v_toks)\n",
        "    print(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}')\n",
        "torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'epoch': epochs, 'src_stoi': src_stoi, 'tgt_stoi': tgt_stoi, 'model_cfg': {'emb': emb_dim, 'hid': hid_dim, 'layers': layers, 'dropout': dropout}}, os.path.join(save_dir, 'checkpoint_last.pt'))\n",
        "print('Saved checkpoint:', os.path.join(save_dir, 'checkpoint_last.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate: Perplexity and BLEU (Public Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loss, val_tok = evaluate_nll(val_loader, model, pad_id_tgt, device); val_ppl = compute_perplexity(val_loss, val_tok)\n",
        "tst_loss, tst_tok = evaluate_nll(test_loader, model, pad_id_tgt, device); tst_ppl = compute_perplexity(tst_loss, tst_tok)\n",
        "bleu = evaluate_bleu(test_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "print(f'Validation perplexity: {val_ppl:.2f}')\n",
        "print(f'Public test perplexity: {tst_ppl:.2f}')\n",
        "print(f'Public test BLEU:       {bleu*100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Private Test (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(private_test_path):\n",
        "    prv_pairs = read_split(private_test_path); prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n",
        "    prv_loader = DataLoader(prv_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "    prv_loss, prv_tok = evaluate_nll(prv_loader, model, pad_id_tgt, device); prv_ppl = compute_perplexity(prv_loss, prv_tok)\n",
        "    prv_bleu = evaluate_bleu(prv_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "    print(f'Private test perplexity: {prv_ppl:.2f}')\n",
        "    print(f'Private test BLEU:       {prv_bleu*100:.2f}')\n",
        "else:\n",
        "    print('Private test split not found at', private_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Predictions for ML‑Arena (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def decode_to_lines(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int) -> List[str]:\n",
        "    lines: List[str] = []\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        pred_ids = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        for b in range(src.size(0)):\n",
        "            hyp = pred_ids[b].tolist()\n",
        "            if eos_id in hyp: hyp = hyp[:hyp.index(eos_id)]\n",
        "            toks = [tgt_itos[i] for i in hyp if i != 0 and i != sos_id]\n",
        "            lines.append(' '.join(toks))\n",
        "    return lines\n",
        "export_split = 'private'; export_format = 'tsv'; export_out = 'submissions/private_predictions.tsv'\n",
        "os.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\n",
        "pairs = read_split(public_test_path if export_split=='public' else private_test_path)\n",
        "exp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi); exp_loader = DataLoader(exp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "preds = decode_to_lines(exp_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "if export_format=='tsv':\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i,h in enumerate(preds): f.write(f'{i}\t{h}\n",
        "')\n",
        "elif export_format=='jsonl':\n",
        "    import json\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i,h in enumerate(preds): f.write(json.dumps({'id': i, 'hyp': h}, ensure_ascii=False)+'\n",
        "')\n",
        "print(f'Wrote {len(preds)} predictions to {export_out}')\n",
        "print('Adjust if ML‑Arena requires a different schema.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}