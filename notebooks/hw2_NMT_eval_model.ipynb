{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-eval",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# HW2 NMT Evaluation\n",
        "Load the trained checkpoint, compute automatic metrics, and inspect sample translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8baa4601",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You chose model 0 - LOL\n"
          ]
        }
      ],
      "source": [
        "MODEL_NR = [0, 1, 2][0]\n",
        "print(f'You chose model {MODEL_NR} - LOL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "setup-utils",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "SPECIAL_TOKENS: Dict[str, str] = {\n",
        "    'pad': '<pad>',\n",
        "    'sos': '<sos>',\n",
        "    'eos': '<eos>',\n",
        "    'unk': '<unk>'\n",
        "}\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    return text.strip().lower().split()\n",
        "\n",
        "def read_split(path: Path) -> List[Tuple[List[str], List[str]]]:\n",
        "    pairs: List[Tuple[List[str], List[str]]] = []\n",
        "    with path.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip('\\n').split('\\t')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n",
        "    return pairs\n",
        "\n",
        "def encode(tokens: List[str], stoi: Dict[str, int], add_sos_eos: bool = False) -> List[int]:\n",
        "    ids = [stoi.get(tok, stoi[SPECIAL_TOKENS['unk']]) for tok in tokens]\n",
        "    if add_sos_eos:\n",
        "        ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']]]\n",
        "    return ids\n",
        "\n",
        "class Example:\n",
        "    def __init__(self, src_ids: List[int], tgt_in_ids: List[int], tgt_out_ids: List[int]):\n",
        "        self.src_ids = src_ids\n",
        "        self.tgt_in_ids = tgt_in_ids\n",
        "        self.tgt_out_ids = tgt_out_ids\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
        "        self.examples: List[Example] = []\n",
        "        for src, tgt in pairs:\n",
        "            src_seq = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
        "            tgt_seq = encode(tgt, tgt_stoi, add_sos_eos=True)\n",
        "            self.examples.append(Example(src_seq, tgt_seq[:-1], tgt_seq[1:]))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Example:\n",
        "        return self.examples[idx]\n",
        "\n",
        "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
        "    src_max = max(len(x.src_ids) for x in batch)\n",
        "    tgt_max = max(len(x.tgt_in_ids) for x in batch)\n",
        "\n",
        "    def pad(seq: List[int], length: int, pad_id: int) -> List[int]:\n",
        "        return seq + [pad_id] * (length - len(seq))\n",
        "\n",
        "    src = torch.tensor([pad(x.src_ids, src_max, pad_id_src) for x in batch])\n",
        "    tgt_in = torch.tensor([pad(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    tgt_out = torch.tensor([pad(x.tgt_out_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    src_l = torch.tensor([len(x.src_ids) for x in batch])\n",
        "    tgt_l = torch.tensor([len(x.tgt_out_ids) for x in batch])\n",
        "    return src, src_l, tgt_in, tgt_out, tgt_l\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "05a99ac4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "HOME_DIR = Path(os.getcwd())\n",
        "WORK_DIR = HOME_DIR / 'DLNLP25W'\n",
        "if os.name == 'posix':\n",
        "    # then create a folder named DLNLP25W\n",
        "    # if folder exists then dont create it\n",
        "    if not os.path.exists(WORK_DIR):\n",
        "        os.makedirs(WORK_DIR)\n",
        "    os.chdir(WORK_DIR)\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metrics-utils",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCS\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    tokens = 0\n",
        "    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n",
        "        src, src_l = src.to(device), src_l.to(device)\n",
        "        tgt_in, tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
        "        logits = model(src, src_l, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        total += float(loss.item())\n",
        "        tokens += int((tgt_out != pad_id_tgt).sum().item())\n",
        "    return total, tokens\n",
        "\n",
        "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
        "    if token_count == 0:\n",
        "        return float('inf')\n",
        "    try:\n",
        "        return float(math.exp(loss_sum / token_count))\n",
        "    except OverflowError:\n",
        "        return float('inf')\n",
        "\n",
        "def corpus_bleu(refs, hyps, max_order: int = 4, smooth: bool = True) -> float:\n",
        "    from collections import Counter\n",
        "\n",
        "    def ngrams(sequence, n):\n",
        "        return Counter(tuple(sequence[i:i + n]) for i in range(len(sequence) - n + 1))\n",
        "\n",
        "    matches = [0] * max_order\n",
        "    possible = [0] * max_order\n",
        "    ref_len = 0\n",
        "    hyp_len = 0\n",
        "\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        ref_len += len(ref)\n",
        "        hyp_len += len(hyp)\n",
        "        for n in range(1, max_order + 1):\n",
        "            ref_ngrams = ngrams(ref, n)\n",
        "            hyp_ngrams = ngrams(hyp, n)\n",
        "            matches[n - 1] += sum(min(count, hyp_ngrams[gram]) for gram, count in ref_ngrams.items())\n",
        "            possible[n - 1] += max(len(hyp) - n + 1, 0)\n",
        "\n",
        "    precisions = []\n",
        "    for m, p in zip(matches, possible):\n",
        "        if smooth:\n",
        "            precisions.append((m + 1) / (p + 1))\n",
        "        else:\n",
        "            precisions.append(m / p if p > 0 else 0.0)\n",
        "\n",
        "    if min(precisions) <= 0:\n",
        "        geo_mean = 0.0\n",
        "    else:\n",
        "        geo_mean = math.exp(sum(math.log(val) for val in precisions) / max_order)\n",
        "\n",
        "    bp = 1.0 if hyp_len > ref_len else math.exp(1 - ref_len / max(1, hyp_len))\n",
        "    return float(geo_mean * bp)\n",
        "\n",
        "def lcs_length(x: List[str], y: List[str]) -> int:\n",
        "    if not x or not y:\n",
        "        return 0\n",
        "    m, n = len(x), len(y)\n",
        "    dp = [0] * (n + 1)\n",
        "    for i in range(1, m + 1):\n",
        "        prev = 0\n",
        "        for j in range(1, n + 1):\n",
        "            tmp = dp[j]\n",
        "            if x[i - 1] == y[j - 1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j - 1])\n",
        "            prev = tmp\n",
        "    return dp[-1]\n",
        "\n",
        "def compute_rouge_l(refs, hyps) -> float:\n",
        "    if not refs:\n",
        "        return 0.0\n",
        "    scores = []\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        if not ref or not hyp:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        l = lcs_length(ref, hyp)\n",
        "        prec = l / len(hyp)\n",
        "        rec = l / len(ref)\n",
        "        if prec + rec == 0:\n",
        "            scores.append(0.0)\n",
        "        else:\n",
        "            scores.append((2 * prec * rec) / (prec + rec))\n",
        "    return float(sum(scores) / len(scores))\n",
        "\n",
        "def ids_to_tokens(ids, itos, pad_id, sos_id=None, eos_id=None):\n",
        "    tokens = []\n",
        "    for idx in ids:\n",
        "        if idx == pad_id:\n",
        "            continue\n",
        "        if eos_id is not None and idx == eos_id:\n",
        "            break\n",
        "        if sos_id is not None and idx == sos_id:\n",
        "            continue\n",
        "        if 0 <= idx < len(itos) and itos[idx] is not None:\n",
        "            tokens.append(itos[idx])\n",
        "    return tokens\n",
        "\n",
        "def build_itos(stoi: Dict[str, int]) -> List[str]:\n",
        "    size = max(stoi.values()) + 1\n",
        "    itos = [None] * size\n",
        "    for token, idx in stoi.items():\n",
        "        if idx >= len(itos):\n",
        "            itos.extend([None] * (idx - len(itos) + 1))\n",
        "        itos[idx] = token\n",
        "    return itos\n",
        "\n",
        "@torch.no_grad()\n",
        "def gather_predictions(loader, model, src_itos, tgt_itos, pad_id_src, pad_id_tgt, sos_id, eos_id, src_eos_id, device, max_len=100, sample_cap=32):\n",
        "    model.eval()\n",
        "    refs, hyps, samples = [], [], []\n",
        "    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n",
        "        src, src_l = src.to(device), src_l.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "        preds = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        preds = preds.cpu()\n",
        "        src = src.cpu()\n",
        "        tgt_out = tgt_out.cpu()\n",
        "        src_l = src_l.cpu()\n",
        "        for b in range(src.size(0)):\n",
        "            src_ids = src[b][:src_l[b]].tolist()\n",
        "            ref_ids = tgt_out[b].tolist()\n",
        "            hyp_ids = preds[b].tolist()\n",
        "            src_tokens = ids_to_tokens(src_ids, src_itos, pad_id_src, eos_id=src_eos_id)\n",
        "            ref_tokens = ids_to_tokens(ref_ids, tgt_itos, pad_id_tgt, sos_id=sos_id, eos_id=eos_id)\n",
        "            hyp_tokens = ids_to_tokens(hyp_ids, tgt_itos, pad_id_tgt, sos_id=sos_id, eos_id=eos_id)\n",
        "            refs.append(ref_tokens)\n",
        "            hyps.append(hyp_tokens)\n",
        "            if len(samples) < sample_cap:\n",
        "                samples.append({\n",
        "                    'src': ' '.join(src_tokens),\n",
        "                    'ref': ' '.join(ref_tokens),\n",
        "                    'hyp': ' '.join(hyp_tokens)\n",
        "                })\n",
        "    return {\n",
        "        'refs': refs,\n",
        "        'hyps': hyps,\n",
        "        'samples': samples\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a803562b",
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODEL_NR == 0:\n",
        "    class Encoder(nn.Module):\n",
        "        def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "            super().__init__()\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "\n",
        "        def forward(self, src, src_lens):\n",
        "            emb = self.emb(src)\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            out, (h, c) = self.rnn(packed)\n",
        "            out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "            return out, (h, c)\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "            super().__init__()\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "            self.proj = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "        def forward(self, tgt_in, hidden):\n",
        "            emb = self.emb(tgt_in)\n",
        "            out, hidden = self.rnn(emb, hidden)\n",
        "            return self.proj(out), hidden\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        def __init__(self, enc, dec):\n",
        "            super().__init__()\n",
        "            self.encoder = enc\n",
        "            self.decoder = dec\n",
        "\n",
        "        def forward(self, src, src_lens, tgt_in):\n",
        "            _, h = self.encoder(src, src_lens)\n",
        "            logits, _ = self.decoder(tgt_in, h)\n",
        "            return logits\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "            B = src.size(0)\n",
        "            _, h = self.encoder(src, src_lens)\n",
        "            inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
        "            outs = []\n",
        "            for _ in range(max_len):\n",
        "                logits, h = self.decoder(inputs[:, -1:].contiguous(), h)\n",
        "                nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "                outs.append(nxt)\n",
        "                inputs = torch.cat([inputs, nxt], dim=1)\n",
        "            \n",
        "            seqs = torch.cat(outs, dim=1)\n",
        "            for i in range(B):\n",
        "                row = seqs[i]\n",
        "                if (row == eos_id).any():\n",
        "                    idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
        "                    row[idx + 1:] = eos_id\n",
        "            return seqs\n",
        "        \n",
        "# MODEL 1: Using GRU, torch.float16 and \n",
        "if MODEL_NR == 1:\n",
        "    class Encoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        pass\n",
        "\n",
        "# MODEL 2: Using GRU, torch.float16, and student-teacher training based on fine-tuned BERT\n",
        "if MODEL_NR == 2:\n",
        "    class Encoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "model-arch",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def build_model(cfg: Dict, src_vocab_size: int, tgt_vocab_size: int) -> nn.Module:\n",
        "    emb_dim = cfg.get('emb', 256)\n",
        "    hid_dim = cfg.get('hid', 512)\n",
        "    layers = cfg.get('layers', 1)\n",
        "    dropout = cfg.get('dropout', 0.1)\n",
        "    encoder = Encoder(src_vocab_size, emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "    decoder = Decoder(tgt_vocab_size, emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "    return Seq2Seq(encoder, decoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "load-ckpt",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint from ../checkpoints/checkpoint_last_0.pt\n"
          ]
        }
      ],
      "source": [
        "checkpoint_candidates = [\n",
        "    Path(f'checkpoints/checkpoint_last_{MODEL_NR}.pt'),\n",
        "    Path(f'../checkpoints/checkpoint_last_{MODEL_NR}.pt'),\n",
        "    Path(f'../../checkpoints/checkpoint_last_{MODEL_NR}.pt')\n",
        "]\n",
        "checkpoint_path = next((p for p in checkpoint_candidates if p.exists()), None)\n",
        "if checkpoint_path is None:\n",
        "    raise FileNotFoundError(f'Could not locate checkpoint_last_{MODEL_NR}.pt in expected directories.')\n",
        "print('Loading checkpoint from', checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5866d0a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built model 0\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model_cfg = checkpoint.get('model_cfg', {})\n",
        "src_stoi = dict(checkpoint['src_stoi'])\n",
        "tgt_stoi = dict(checkpoint['tgt_stoi'])\n",
        "src_vocab_size = max(src_stoi.values()) + 1\n",
        "tgt_vocab_size = max(tgt_stoi.values()) + 1\n",
        "model = build_model(model_cfg, src_vocab_size, tgt_vocab_size).to(device)\n",
        "model.load_state_dict(checkpoint['model_state'])\n",
        "model.eval()\n",
        "\n",
        "print(f'Built model {MODEL_NR}')\n",
        "\n",
        "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]\n",
        "pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
        "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]\n",
        "eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
        "src_eos_id = src_stoi[SPECIAL_TOKENS['eos']]\n",
        "\n",
        "src_itos = build_itos(src_stoi)\n",
        "tgt_itos = build_itos(tgt_stoi)\n",
        "max_decode_len = checkpoint.get('max_decode_len', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "data-loaders",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation examples: 32428\n",
            "Public_Test examples: 32428\n"
          ]
        }
      ],
      "source": [
        "data_dir_candidates = [\n",
        "    Path('../data'),\n",
        "    Path('../../data'),\n",
        "    Path('../dataset_splits'),\n",
        "    Path('../../dataset_splits')\n",
        "]\n",
        "\n",
        "def resolve_split(filename: str) -> Path:\n",
        "    for base in data_dir_candidates:\n",
        "        candidate = (base / filename).resolve()\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    raise FileNotFoundError(f'Could not find {filename} in expected data directories.')\n",
        "\n",
        "split_files = {\n",
        "    'validation': 'val.txt',\n",
        "    'public_test': 'public_test.txt'\n",
        "}\n",
        "\n",
        "datasets = {}\n",
        "for split_name, filename in split_files.items():\n",
        "    path = resolve_split(filename)\n",
        "    pairs = read_split(path)\n",
        "    datasets[split_name] = TranslationDataset(pairs, src_stoi, tgt_stoi)\n",
        "    print(f'{split_name.title()} examples: {len(datasets[split_name])}')\n",
        "\n",
        "batch_size = 64\n",
        "collate_fn = lambda batch: collate_pad(batch, pad_id_src, pad_id_tgt)\n",
        "loaders = {\n",
        "    split: DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "    for split, ds in datasets.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "run-metrics",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation =>\t Perplexity: 6.15 | BLEU: 25.19 | ROUGE-L: 53.60\n",
            "Public_Test =>\t Perplexity: 6.11 | BLEU: 25.50 | ROUGE-L: 53.84\n"
          ]
        }
      ],
      "source": [
        "metrics_summary = {}\n",
        "for split_name, loader in loaders.items():\n",
        "    loss, tokens = evaluate_nll(loader, model, pad_id_tgt, device)\n",
        "    ppl = compute_perplexity(loss, tokens)\n",
        "    decoded = gather_predictions(\n",
        "        loader, model, src_itos, tgt_itos,\n",
        "        pad_id_src, pad_id_tgt, sos_id, eos_id, src_eos_id,\n",
        "        device, max_len=max_decode_len, sample_cap=64\n",
        "    )\n",
        "    bleu = corpus_bleu(decoded['refs'], decoded['hyps']) * 100\n",
        "    rouge = compute_rouge_l(decoded['refs'], decoded['hyps']) * 100\n",
        "    metrics_summary[split_name] = {\n",
        "        'perplexity': ppl,\n",
        "        'bleu': bleu,\n",
        "        'rouge_l': rouge,\n",
        "        'samples': decoded['samples']\n",
        "    }\n",
        "    print(f\"{split_name.title()} =>\\t Perplexity: {ppl:.2f} | BLEU: {bleu:.2f} | ROUGE-L: {rouge:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "show-samples",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing 10 example translations from the validation split:\n",
            "[1] SRC: she turned around when she heard his voice.\n",
            "    REF: sie drehte sich um, als sie seine stimme hörte.\n",
            "    HYP: sie drehte sich um die <unk> als sie sich zu küssen.\n",
            "[2] SRC: do you remember the first time we went to boston together?\n",
            "    REF: erinnerst du dich an das erste mal, dass wir gemeinsam nach boston <unk>\n",
            "    HYP: weißt du noch, wie wir das erstemal um uns noch nach boston gegangen ist?\n",
            "[3] SRC: he is old.\n",
            "    REF: er ist alt.\n",
            "    HYP: er ist alt.\n",
            "[4] SRC: i can still remember a few french words.\n",
            "    REF: ein paar worte auf französisch kann ich noch.\n",
            "    HYP: ich kann noch immer noch ein paar französische lieder singen.\n",
            "[5] SRC: we helped him.\n",
            "    REF: wir halfen ihm.\n",
            "    HYP: wir haben ihm geholfen.\n",
            "[6] SRC: you should've never come here.\n",
            "    REF: sie hätten niemals hierherkommen sollen.\n",
            "    HYP: du hättest nie hierherkommen sollen.\n",
            "[7] SRC: did you buy what you wanted?\n",
            "    REF: habt ihr das, was ihr <unk> gekauft?\n",
            "    HYP: hast du darauf geachtet, was du wolltest?\n",
            "[8] SRC: what time do you usually leave home?\n",
            "    REF: um welche zeit gehst du gewöhnlich von zuhause fort?\n",
            "    HYP: um wieviel uhr fahren sie sich nach hause?\n",
            "[9] SRC: don't play with me.\n",
            "    REF: spiel nicht mit mir.\n",
            "    HYP: spiel nicht mit mir auf.\n",
            "[10] SRC: i'm not going to fire you.\n",
            "    REF: ich werde sie nicht entlassen.\n",
            "    HYP: ich werde dich nicht entlassen.\n"
          ]
        }
      ],
      "source": [
        "example_split = 'validation' if 'validation' in metrics_summary else next(iter(metrics_summary))\n",
        "samples = metrics_summary[example_split]['samples'][:10]\n",
        "\n",
        "print(\n",
        "    f'Showing {len(samples)} example translations from the {example_split} split:'\n",
        ")\n",
        "\n",
        "for idx, sample in enumerate(samples, 1):\n",
        "    print(f'[{idx}] SRC: {sample[\"src\"]}')\n",
        "    print(f'    REF: {sample[\"ref\"]}')\n",
        "    print(f'    HYP: {sample[\"hyp\"]}')\n",
        "\n",
        "if len(samples) < 10:\n",
        "    print('Fewer than 10 examples available in this split.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7dcc53c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
