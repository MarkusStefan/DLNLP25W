{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NMT Homework (Self-Contained): EN→DE\n",
        "\n",
        "Train a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n",
        "\n",
        "Focus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n",
        "\n",
        "Data: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "Use `install.sh` or `pip install -r requirements.txt` to set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e6b75ec3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You chose model 0 - LOL\n"
          ]
        }
      ],
      "source": [
        "MODEL_NR = [0, 1, 2][0]\n",
        "print(f'You chose model {MODEL_NR} - LOL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu128\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# !pip install -r requirements.txt\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "try: \n",
        "    sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception: \n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "83dd41f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "HOME_DIR = Path(os.getcwd())\n",
        "WORK_DIR = HOME_DIR / 'DLNLP25W'\n",
        "if os.name == 'posix':\n",
        "    # then create a folder named DLNLP25W\n",
        "    # if folder exists then dont create it\n",
        "    if not os.path.exists(WORK_DIR):\n",
        "        os.makedirs(WORK_DIR)\n",
        "    os.chdir(WORK_DIR)\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Shared Utilities (no external imports)\n",
        "Tokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "SPECIAL_TOKENS = {'pad': '<pad>', 'sos': '<sos>', 'eos': '<eos>', 'unk': '<unk>'}\n",
        "\n",
        "def simple_tokenize(s: str) -> List[str]:\n",
        "    return s.strip().lower().split()\n",
        "\n",
        "def read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n",
        "    pairs: List[Tuple[List[str], List[str]]] = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip(' ').split('\\t')\n",
        "            if len(parts) < 2: \n",
        "                continue\n",
        "            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n",
        "    return pairs\n",
        "\n",
        "def build_vocab(seqs: Iterable[List[str]], max_size: int | None = None) -> Dict[str, int]:\n",
        "    from collections import Counter\n",
        "    c = Counter()\n",
        "    for s in seqs: \n",
        "        c.update(s)\n",
        "    itms = c.most_common(max_size) if max_size else c.items()\n",
        "    stoi = { # special tokens ids\n",
        "        SPECIAL_TOKENS['pad']: 0, \n",
        "        SPECIAL_TOKENS['sos']: 1, \n",
        "        SPECIAL_TOKENS['eos']: 2, \n",
        "        SPECIAL_TOKENS['unk']: 3,\n",
        "    }\n",
        "    for w, _ in itms:\n",
        "        if w not in stoi: \n",
        "            stoi[w] = len(stoi)\n",
        "    return stoi\n",
        "\n",
        "def encode(tokens: List[str], stoi: Dict[str, int], add_sos_eos: bool = False) -> List[int]:\n",
        "    ids = [stoi.get(t, stoi[SPECIAL_TOKENS['unk']]) for t in tokens]\n",
        "    if add_sos_eos: \n",
        "        ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']]]\n",
        "    return ids\n",
        "\n",
        "class Example:\n",
        "    def __init__(self, s: List[int], ti: List[int], to: List[int]): \n",
        "        self.src_ids = s\n",
        "        self.tgt_in_ids = ti\n",
        "        self.tgt_out_ids = to\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
        "        self.examples: List[Example] = []\n",
        "        for src, tgt in pairs:\n",
        "            s = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
        "            t = encode(tgt, tgt_stoi, add_sos_eos=True)\n",
        "            self.examples.append(Example(s, t[:-1], t[1:]))\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, i): \n",
        "        return self.examples[i]\n",
        "\n",
        "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
        "    src_max = max(len(x.src_ids) for x in batch)\n",
        "    tgt_max = max(len(x.tgt_in_ids) for x in batch)\n",
        "    \n",
        "    def pad_to(a, L, pad):\n",
        "        return a + [pad] * (L - len(a))\n",
        "        \n",
        "    src = torch.tensor([pad_to(x.src_ids, src_max, pad_id_src) for x in batch])\n",
        "    tgt_in = torch.tensor([pad_to(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    tgt_out = torch.tensor([pad_to(x.tgt_out_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    src_l = torch.tensor([len(x.src_ids) for x in batch])\n",
        "    tgt_l = torch.tensor([len(x.tgt_out_ids) for x in batch])\n",
        "    return src, src_l, tgt_in, tgt_out, tgt_l\n",
        "\n",
        "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
        "    if token_count == 0:\n",
        "        return float('inf')\n",
        "    try:\n",
        "        return float(math.exp(loss_sum / token_count))\n",
        "    except OverflowError:\n",
        "        return float('inf')\n",
        "\n",
        "def corpus_bleu(refs: List[List[str]], hyps: List[List[str]], max_order: int = 4, smooth: bool = True) -> float:\n",
        "    from collections import Counter\n",
        "\n",
        "    def ngrams(t, n):\n",
        "        return Counter([tuple(t[i:i+n]) for i in range(len(t)-n+1)])\n",
        "\n",
        "    m = [0] * max_order\n",
        "    p = [0] * max_order\n",
        "    rl = 0\n",
        "    hl = 0\n",
        "\n",
        "    for r, h in zip(refs, hyps):\n",
        "        rl += len(r)\n",
        "        hl += len(h)\n",
        "        for n in range(1, max_order + 1):\n",
        "            R = ngrams(r, n)\n",
        "            H = ngrams(h, n)\n",
        "            m[n-1] += sum(min(c, H[g]) for g, c in R.items())\n",
        "            p[n-1] += max(len(h) - n + 1, 0)\n",
        "\n",
        "    prec = [\n",
        "        (m[i] + 1) / (p[i] + 1) if smooth else (m[i] / p[i] if p[i] > 0 else 0.0)\n",
        "        for i in range(max_order)\n",
        "    ]\n",
        "    geo = math.exp(sum((1 / max_order) * math.log(x) for x in prec if x > 0)) if min(prec) > 0 else 0.0\n",
        "    bp = 1.0 if hl > rl else math.exp(1 - rl / max(1, hl))\n",
        "    return float(geo * bp)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n",
        "    '''Evaluation of negative log-likelihood loss on the given data loader.'''\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "    model.eval()\n",
        "    tot = 0.0\n",
        "    toks = 0\n",
        "    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n",
        "        src, src_l = src.to(device), src_l.to(device)\n",
        "        tgt_in, tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
        "        logits = model(src, src_l, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        tot += float(loss.item())\n",
        "        toks += int((tgt_out != pad_id_tgt).sum().item())\n",
        "    return tot, toks\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bleu(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int = 100):\n",
        "    '''BLEU score evaluation on the given data loader.'''\n",
        "    model.eval()\n",
        "    refs = []\n",
        "    hyps = []\n",
        "    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n",
        "        src, src_l = src.to(device), src_l.to(device)\n",
        "        pred = model.greedy_decode(\n",
        "            src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id\n",
        "        )\n",
        "        for b in range(src.size(0)):\n",
        "            ref_ids = tgt_out[b].tolist()\n",
        "            hyp_ids = pred[b].tolist()\n",
        "            if eos_id in ref_ids: \n",
        "                ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
        "            if eos_id in hyp_ids: \n",
        "                hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
        "            refs.append([tgt_itos[i] for i in ref_ids if i != 0])\n",
        "            hyps.append([tgt_itos[i] for i in hyp_ids if i != 0 and i != sos_id])\n",
        "    return float(corpus_bleu(refs, hyps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Paths and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public test path: ../data/public_test.txt\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "\n",
        "if not os.path.exists('../data'):\n",
        "    import requests\n",
        "    os.makedirs('../data', exist_ok=True)\n",
        "    # downnload data\n",
        "    for dataset in ['train', 'val', 'public_test',]:\n",
        "        url = f'https://raw.githubusercontent.com/MarkusStefan/DLNLP25W/dev/data/{dataset}.txt'\n",
        "        response = requests.get(url)\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        with open(f'../data/{dataset}.txt', 'w', encoding='utf-8') as f:\n",
        "            for line in lines:\n",
        "                f.write(line + '\\n')\n",
        "    train_path = '../data/train.txt'\n",
        "    val_path = '../data/val.txt'\n",
        "    public_test_path = '../data/public_test.txt'\n",
        "\n",
        "if not os.path.exists(public_test_path):\n",
        "    alt = '../data/test_public.txt'\n",
        "    public_test_path = alt if os.path.exists(alt) else public_test_path\n",
        "\n",
        "private_test_path = '../data/private_test.txt'\n",
        "\n",
        "src_vocab_size = 30000\n",
        "tgt_vocab_size = 30000\n",
        "emb_dim = 256\n",
        "hid_dim = 512\n",
        "layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "lr = 3e-4\n",
        "max_decode_len = 100\n",
        "\n",
        "save_dir = 'checkpoints'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "SAVE_DIR = os.path.join(save_dir, f'checkpoint_last_{MODEL_NR}.pt')\n",
        "print('Public test path:', public_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading splits...\n",
            "Train: 226,997 | Val: 32,428 | Public test: 32,428\n",
            "Vocab sizes — src: 30004 tgt: 30004\n"
          ]
        }
      ],
      "source": [
        "print('Loading splits...')\n",
        "train_pairs = read_split(train_path)\n",
        "val_pairs = read_split(val_path)\n",
        "test_pairs = read_split(public_test_path)\n",
        "print(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\n",
        "\n",
        "src_stoi = build_vocab((s for s, _ in train_pairs), max_size=src_vocab_size)\n",
        "tgt_stoi = build_vocab((t for _, t in train_pairs), max_size=tgt_vocab_size)\n",
        "\n",
        "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]\n",
        "pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
        "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]\n",
        "eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
        "\n",
        "train_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi)\n",
        "val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi)\n",
        "test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\n",
        "\n",
        "collate = lambda b: collate_pad(b, pad_id_src, pad_id_tgt)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "\n",
        "tgt_itos = [None] * len(tgt_stoi)\n",
        "for w, i in tgt_stoi.items():\n",
        "    if 0 <= i < len(tgt_itos):\n",
        "        tgt_itos[i] = w\n",
        "\n",
        "print('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Model (Your Playground)\n",
        "Keep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODEL_NR == 0:\n",
        "    class Encoder(nn.Module):\n",
        "        def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "            super().__init__()\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "\n",
        "        def forward(self, src, src_lens):\n",
        "            emb = self.emb(src)\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            out, (h, c) = self.rnn(packed)\n",
        "            out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "            return out, (h, c)\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "            super().__init__()\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "            self.proj = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "        def forward(self, tgt_in, hidden):\n",
        "            emb = self.emb(tgt_in)\n",
        "            out, hidden = self.rnn(emb, hidden)\n",
        "            return self.proj(out), hidden\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        def __init__(self, enc, dec):\n",
        "            super().__init__()\n",
        "            self.encoder = enc\n",
        "            self.decoder = dec\n",
        "\n",
        "        def forward(self, src, src_lens, tgt_in):\n",
        "            _, h = self.encoder(src, src_lens)\n",
        "            logits, _ = self.decoder(tgt_in, h)\n",
        "            return logits\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "            B = src.size(0)\n",
        "            _, h = self.encoder(src, src_lens)\n",
        "            inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
        "            outs = []\n",
        "            for _ in range(max_len):\n",
        "                logits, h = self.decoder(inputs[:, -1:].contiguous(), h)\n",
        "                nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "                outs.append(nxt)\n",
        "                inputs = torch.cat([inputs, nxt], dim=1)\n",
        "            \n",
        "            seqs = torch.cat(outs, dim=1)\n",
        "            for i in range(B):\n",
        "                row = seqs[i]\n",
        "                if (row == eos_id).any():\n",
        "                    idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
        "                    row[idx + 1:] = eos_id\n",
        "            return seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2f910fb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL 1: Using GRU, torch.float16 and \n",
        "if MODEL_NR == 1:\n",
        "    class Encoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4a8934e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL 2: Using GRU, torch.float16, and student-teacher training based on fine-tuned BERT\n",
        "if MODEL_NR == 2:\n",
        "    class Encoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Decoder(nn.Module):\n",
        "        pass\n",
        "\n",
        "\n",
        "    class Seq2Seq(nn.Module):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "48566434",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nr of params:\t 33908020\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "print(\"Nr of params:\\t\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [05:27<00:00, 65.59s/it, train_ppl=3.86, val_ppl=6.15]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: checkpoints/checkpoint_last_0.pt\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "evals = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_perplexity': [],\n",
        "    'val_perplexity': [],\n",
        "    'train_nll': [],\n",
        "    'val_nll': [],\n",
        "\n",
        "}\n",
        "with tqdm(total=epochs) as pbar:\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tot = 0.0\n",
        "        toks = 0\n",
        "        n_samples = train_loader.dataset.__len__()\n",
        "        n_samples_seen = 0\n",
        "        for src, src_l, tgt_in, tgt_out, tgt_l in train_loader:\n",
        "            src, src_l = src.to(device), src_l.to(device)\n",
        "            tgt_in, tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(src, src_l, tgt_in)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            tot += float(loss.item())\n",
        "            toks += int((tgt_out != pad_id_tgt).sum().item())\n",
        "            n_samples_seen += src.size(0)\n",
        "            pbar.set_postfix(\n",
        "                train_loss=f'{tot / toks if toks > 0 else float(\"inf\"):.2f}', \n",
        "                epoch_pct=f'{(n_samples_seen / n_samples) * 100:.2f}', \n",
        "                overall_train_pct=f'{(epoch * n_samples + n_samples_seen) / (epochs * n_samples) * 100:.2f}'\n",
        "            )\n",
        "            \n",
        "        tr_ppl = compute_perplexity(tot, toks)\n",
        "        v_loss, v_toks = evaluate_nll(val_loader, model, pad_id_tgt, device)\n",
        "        v_ppl = compute_perplexity(v_loss, v_toks)\n",
        "        # update tqdm summary metrics\n",
        "        pbar.set_postfix(train_ppl=f'{tr_ppl:.2f}', val_ppl=f'{v_ppl:.2f}')\n",
        "        pbar.update(1)\n",
        "\n",
        "torch.save({\n",
        "    'model_state': model.state_dict(),\n",
        "    'optimizer_state': optimizer.state_dict(),\n",
        "    'epoch': epochs,\n",
        "    'src_stoi': src_stoi,\n",
        "    'tgt_stoi': tgt_stoi,\n",
        "    'model_cfg': {'emb': emb_dim, 'hid': hid_dim, 'layers': layers, 'dropout': dropout}\n",
        "}, SAVE_DIR)\n",
        "\n",
        "print('Saved checkpoint:', SAVE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate: Perplexity and BLEU (Public Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a1d53e",
      "metadata": {},
      "source": [
        "BLEU score:\n",
        "$$\n",
        "  B(\\hat{y}, y) = \\beta \\cdot \\exp\\left(\\sum_{i=1}^N w_i \\cdot \\ln(p_i)\\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation perplexity:  6.15\n",
            "Public test perplexity: 6.11\n",
            "Validation BLEU:        25.19\n",
            "Public test BLEU:       25.50\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_tok = evaluate_nll(val_loader, model, pad_id_tgt, device)\n",
        "val_ppl = compute_perplexity(val_loss, val_tok)\n",
        "\n",
        "tst_loss, tst_tok = evaluate_nll(test_loader, model, pad_id_tgt, device)\n",
        "tst_ppl = compute_perplexity(tst_loss, tst_tok)\n",
        "\n",
        "val_bleu = evaluate_bleu(val_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "bleu = evaluate_bleu(test_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "\n",
        "print(f'Validation perplexity:  {val_ppl:.2f}')\n",
        "print(f'Public test perplexity: {tst_ppl:.2f}')\n",
        "print(f'Validation BLEU:        {val_bleu * 100:.2f}')\n",
        "print(f'Public test BLEU:       {bleu * 100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Private Test (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private test split not found at ../data/private_test.txt\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(private_test_path):\n",
        "    prv_pairs = read_split(private_test_path)\n",
        "    prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n",
        "    prv_loader = DataLoader(prv_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "    \n",
        "    prv_loss, prv_tok = evaluate_nll(prv_loader, model, pad_id_tgt, device)\n",
        "    prv_ppl = compute_perplexity(prv_loss, prv_tok)\n",
        "    \n",
        "    prv_bleu = evaluate_bleu(prv_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "    \n",
        "    print(f'Private test perplexity: {prv_ppl:.2f}')\n",
        "    print(f'Private test BLEU:       {prv_bleu * 100:.2f}')\n",
        "else:\n",
        "    print('Private test split not found at', private_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Predictions for ML‑Arena (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 32428 predictions to submissions/private_predictions.tsv\n",
            "Adjust if ML‑Arena requires a different schema.\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def decode_to_lines(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int) -> List[str]:\n",
        "    lines: List[str] = []\n",
        "    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n",
        "        src, src_l = src.to(device), src_l.to(device)\n",
        "        pred_ids = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        for b in range(src.size(0)):\n",
        "            hyp = pred_ids[b].tolist()\n",
        "            if eos_id in hyp:\n",
        "                hyp = hyp[:hyp.index(eos_id)]\n",
        "            toks = [tgt_itos[i] for i in hyp if i != 0 and i != sos_id]\n",
        "            lines.append(' '.join(toks))\n",
        "    return lines\n",
        "\n",
        "# export_split = 'private'\n",
        "export_split = 'public'\n",
        "export_format = 'tsv'\n",
        "export_out = 'submissions/private_predictions.tsv'\n",
        "\n",
        "os.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\n",
        "pairs = read_split(public_test_path if export_split == 'public' else private_test_path)\n",
        "exp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi)\n",
        "exp_loader = DataLoader(exp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "\n",
        "preds = decode_to_lines(exp_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "\n",
        "if export_format == 'tsv':\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i, h in enumerate(preds):\n",
        "            f.write(f'{i}\\t{h}\\n')\n",
        "elif export_format == 'jsonl':\n",
        "    import json\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i, h in enumerate(preds):\n",
        "            f.write(json.dumps({'id': i, 'hyp': h}, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f'Wrote {len(preds)} predictions to {export_out}')\n",
        "print('Adjust if ML‑Arena requires a different schema.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
