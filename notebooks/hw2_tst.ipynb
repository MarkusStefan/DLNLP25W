{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NMT Homework (Self-Contained): EN→DE\n",
        "\n",
        "Train a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n",
        "\n",
        "Focus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n",
        "\n",
        "Data: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "Use `install.sh` or `pip install -r requirements.txt` to set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu128\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch tqdm\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "try:\n",
        "    sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec4a34b",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Shared Utilities (no external imports)\n",
        "Tokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Iterable\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SPECIAL_TOKENS = {\n",
        "    'pad': '<pad>',\n",
        "    'sos': '<sos>',\n",
        "    'eos': '<eos>',\n",
        "    'unk': '<unk>'\n",
        "}\n",
        "\n",
        "\n",
        "def simple_tokenize(s: str) -> List[str]:\n",
        "    \"\"\"Lowercase whitespace tokenizer.\"\"\"\n",
        "    return s.strip().lower().split()\n",
        "\n",
        "\n",
        "def read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n",
        "    \"\"\"Read tab-separated translation pairs from file.\"\"\"\n",
        "    pairs: List[Tuple[List[str], List[str]]] = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip('\\n').split('\\t')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            pairs.append((\n",
        "                simple_tokenize(parts[0]),\n",
        "                simple_tokenize(parts[1])\n",
        "            ))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def build_vocab(\n",
        "    seqs: Iterable[List[str]],\n",
        "    max_size: int | None = None\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"Build vocabulary from token sequences.\"\"\"\n",
        "    counter = Counter()\n",
        "    for seq in seqs:\n",
        "        counter.update(seq)\n",
        "    \n",
        "    items = counter.most_common(max_size) if max_size else counter.items()\n",
        "    \n",
        "    # Initialize with special tokens\n",
        "    stoi = {\n",
        "        SPECIAL_TOKENS['pad']: 0,\n",
        "        SPECIAL_TOKENS['sos']: 1,\n",
        "        SPECIAL_TOKENS['eos']: 2,\n",
        "        SPECIAL_TOKENS['unk']: 3\n",
        "    }\n",
        "    \n",
        "    for word, _ in items:\n",
        "        if word not in stoi:\n",
        "            stoi[word] = len(stoi)\n",
        "    \n",
        "    return stoi\n",
        "\n",
        "\n",
        "def encode(\n",
        "    tokens: List[str],\n",
        "    stoi: Dict[str, int],\n",
        "    add_sos_eos: bool = False\n",
        ") -> List[int]:\n",
        "    \"\"\"Encode tokens to indices using vocabulary.\"\"\"\n",
        "    ids = [stoi.get(token, stoi[SPECIAL_TOKENS['unk']]) for token in tokens]\n",
        "    \n",
        "    if add_sos_eos:\n",
        "        ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']]]\n",
        "    \n",
        "    return ids\n",
        "\n",
        "\n",
        "class Example:\n",
        "    \"\"\"Container for a single translation example.\"\"\"\n",
        "    def __init__(self, src_ids: List[int], tgt_in_ids: List[int], tgt_out_ids: List[int]):\n",
        "        self.src_ids = src_ids\n",
        "        self.tgt_in_ids = tgt_in_ids\n",
        "        self.tgt_out_ids = tgt_out_ids\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Dataset for translation pairs.\"\"\"\n",
        "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
        "        self.examples: List[Example] = []\n",
        "        \n",
        "        for src_tokens, tgt_tokens in pairs:\n",
        "            # Source: tokens + EOS\n",
        "            src_ids = encode(src_tokens, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
        "            \n",
        "            # Target: SOS + tokens + EOS\n",
        "            tgt_ids = encode(tgt_tokens, tgt_stoi, add_sos_eos=True)\n",
        "            \n",
        "            # Decoder input: SOS + tokens, Decoder output: tokens + EOS\n",
        "            self.examples.append(Example(src_ids, tgt_ids[:-1], tgt_ids[1:]))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
        "    \"\"\"Collate function with padding for batching.\"\"\"\n",
        "    src_max = max(len(ex.src_ids) for ex in batch)\n",
        "    tgt_max = max(len(ex.tgt_in_ids) for ex in batch)\n",
        "    \n",
        "    def pad_to(seq, length, pad_value):\n",
        "        return seq + [pad_value] * (length - len(seq))\n",
        "    \n",
        "    src = torch.tensor([\n",
        "        pad_to(ex.src_ids, src_max, pad_id_src) for ex in batch\n",
        "    ])\n",
        "    tgt_in = torch.tensor([\n",
        "        pad_to(ex.tgt_in_ids, tgt_max, pad_id_tgt) for ex in batch\n",
        "    ])\n",
        "    tgt_out = torch.tensor([\n",
        "        pad_to(ex.tgt_out_ids, tgt_max, pad_id_tgt) for ex in batch\n",
        "    ])\n",
        "    src_lens = torch.tensor([len(ex.src_ids) for ex in batch])\n",
        "    tgt_lens = torch.tensor([len(ex.tgt_out_ids) for ex in batch])\n",
        "    \n",
        "    return src, src_lens, tgt_in, tgt_out, tgt_lens\n",
        "\n",
        "\n",
        "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
        "    \"\"\"Compute perplexity from total loss and token count.\"\"\"\n",
        "    if token_count == 0:\n",
        "        return float('inf')\n",
        "    try:\n",
        "        return float(math.exp(loss_sum / token_count))\n",
        "    except OverflowError:\n",
        "        return float('inf')\n",
        "\n",
        "\n",
        "def corpus_bleu(\n",
        "    refs: List[List[str]],\n",
        "    hyps: List[List[str]],\n",
        "    max_order: int = 4,\n",
        "    smooth: bool = True\n",
        ") -> float:\n",
        "    \"\"\"Compute corpus-level BLEU score.\"\"\"\n",
        "    def get_ngrams(tokens, n):\n",
        "        return Counter([tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
        "    \n",
        "    matches = [0] * max_order\n",
        "    possible = [0] * max_order\n",
        "    ref_length = 0\n",
        "    hyp_length = 0\n",
        "    \n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        ref_length += len(ref)\n",
        "        hyp_length += len(hyp)\n",
        "        \n",
        "        for n in range(1, max_order + 1):\n",
        "            ref_ngrams = get_ngrams(ref, n)\n",
        "            hyp_ngrams = get_ngrams(hyp, n)\n",
        "            \n",
        "            matches[n-1] += sum(\n",
        "                min(count, hyp_ngrams[ngram])\n",
        "                for ngram, count in ref_ngrams.items()\n",
        "            )\n",
        "            possible[n-1] += max(len(hyp) - n + 1, 0)\n",
        "    \n",
        "    # Compute precision for each n-gram order\n",
        "    if smooth:\n",
        "        precisions = [\n",
        "            (matches[i] + 1) / (possible[i] + 1)\n",
        "            for i in range(max_order)\n",
        "        ]\n",
        "    else:\n",
        "        precisions = [\n",
        "            matches[i] / possible[i] if possible[i] > 0 else 0.0\n",
        "            for i in range(max_order)\n",
        "        ]\n",
        "    \n",
        "    # Geometric mean of precisions\n",
        "    if min(precisions) > 0:\n",
        "        geo_mean = math.exp(\n",
        "            sum((1 / max_order) * math.log(p) for p in precisions)\n",
        "        )\n",
        "    else:\n",
        "        geo_mean = 0.0\n",
        "    \n",
        "    # Brevity penalty\n",
        "    if hyp_length > ref_length:\n",
        "        bp = 1.0\n",
        "    else:\n",
        "        bp = math.exp(1 - ref_length / max(1, hyp_length))\n",
        "    \n",
        "    return float(geo_mean * bp)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_nll(\n",
        "    loader: DataLoader,\n",
        "    model: nn.Module,\n",
        "    pad_id_tgt: int,\n",
        "    device: torch.device\n",
        "):\n",
        "    \"\"\"Evaluate negative log-likelihood on a dataset.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    for src, src_lens, tgt_in, tgt_out, tgt_lens in loader:\n",
        "        src = src.to(device)\n",
        "        src_lens = src_lens.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "        \n",
        "        logits = model(src, src_lens, tgt_in)\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt_out.reshape(-1)\n",
        "        )\n",
        "        \n",
        "        total_loss += float(loss.item())\n",
        "        total_tokens += int((tgt_out != pad_id_tgt).sum().item())\n",
        "    \n",
        "    return total_loss, total_tokens\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bleu(\n",
        "    loader: DataLoader,\n",
        "    model: nn.Module,\n",
        "    tgt_itos: List[str],\n",
        "    sos_id: int,\n",
        "    eos_id: int,\n",
        "    device: torch.device,\n",
        "    max_len: int = 100\n",
        "):\n",
        "    \"\"\"Evaluate BLEU score on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    \n",
        "    for src, src_lens, tgt_in, tgt_out, tgt_lens in loader:\n",
        "        src = src.to(device)\n",
        "        src_lens = src_lens.to(device)\n",
        "        \n",
        "        predictions = model.greedy_decode(\n",
        "            src, src_lens,\n",
        "            max_len=max_len,\n",
        "            sos_id=sos_id,\n",
        "            eos_id=eos_id\n",
        "        )\n",
        "        \n",
        "        for b in range(src.size(0)):\n",
        "            ref_ids = tgt_out[b].tolist()\n",
        "            hyp_ids = predictions[b].tolist()\n",
        "            \n",
        "            # Truncate at EOS\n",
        "            if eos_id in ref_ids:\n",
        "                ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
        "            if eos_id in hyp_ids:\n",
        "                hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
        "            \n",
        "            # Convert to tokens, filter padding\n",
        "            ref_tokens = [tgt_itos[i] for i in ref_ids if i != 0]\n",
        "            hyp_tokens = [tgt_itos[i] for i in hyp_ids if i != 0 and i != sos_id]\n",
        "            \n",
        "            references.append(ref_tokens)\n",
        "            hypotheses.append(hyp_tokens)\n",
        "    \n",
        "    return float(corpus_bleu(references, hypotheses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Paths and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Data paths\n",
        "train_path = 'dataset_splits/train.txt'\n",
        "val_path = 'dataset_splits/val.txt'\n",
        "public_test_path = 'dataset_splits/public_test.txt'\n",
        "\n",
        "# Fallback for alternative naming\n",
        "if not os.path.exists(public_test_path):\n",
        "    alt = 'dataset_splits/test_public.txt'\n",
        "    public_test_path = alt if os.path.exists(alt) else public_test_path\n",
        "\n",
        "private_test_path = 'dataset_splits/private_test.txt'\n",
        "\n",
        "# Vocabulary sizes\n",
        "src_vocab_size = 30000\n",
        "tgt_vocab_size = 30000\n",
        "\n",
        "# Model hyperparameters\n",
        "emb_dim = 256\n",
        "hid_dim = 512\n",
        "layers = 1\n",
        "dropout = 0.1\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "lr = 3e-4\n",
        "max_decode_len = 100\n",
        "\n",
        "# Checkpoint directory\n",
        "save_dir = 'checkpoints'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print('Public test path:', public_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Loading splits...')\n",
        "train_pairs = read_split(train_path)\n",
        "val_pairs = read_split(val_path)\n",
        "test_pairs = read_split(public_test_path)\n",
        "\n",
        "print(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\n",
        "\n",
        "# Build vocabularies\n",
        "src_stoi = build_vocab(\n",
        "    (src for src, _ in train_pairs),\n",
        "    max_size=src_vocab_size\n",
        ")\n",
        "tgt_stoi = build_vocab(\n",
        "    (tgt for _, tgt in train_pairs),\n",
        "    max_size=tgt_vocab_size\n",
        ")\n",
        "\n",
        "# Special token IDs\n",
        "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]\n",
        "pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
        "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]\n",
        "eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
        "\n",
        "# Create datasets\n",
        "train_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi)\n",
        "val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi)\n",
        "test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\n",
        "\n",
        "# Create dataloaders\n",
        "collate = lambda batch: collate_pad(batch, pad_id_src, pad_id_tgt)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    num_workers=0\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Create inverse vocabulary for target (index to string)\n",
        "tgt_itos = [None] * len(tgt_stoi)\n",
        "for word, idx in tgt_stoi.items():\n",
        "    if 0 <= idx < len(tgt_itos):\n",
        "        tgt_itos[idx] = word\n",
        "\n",
        "print('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Model (Your Playground)\n",
        "Keep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"LSTM encoder with packed sequences.\"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim,\n",
        "            hid_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "    \n",
        "    def forward(self, src, src_lens):\n",
        "        # Embed tokens\n",
        "        embedded = self.embedding(src)\n",
        "        \n",
        "        # Pack sequence for efficiency\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded,\n",
        "            src_lens.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        \n",
        "        # Process through LSTM\n",
        "        outputs, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        # Unpack sequence\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        \n",
        "        return outputs, (hidden, cell)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"LSTM decoder with output projection.\"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim,\n",
        "            hid_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.projection = nn.Linear(hid_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, tgt_in, hidden):\n",
        "        # Embed tokens\n",
        "        embedded = self.embedding(tgt_in)\n",
        "        \n",
        "        # Process through LSTM\n",
        "        outputs, hidden = self.rnn(embedded, hidden)\n",
        "        \n",
        "        # Project to vocabulary\n",
        "        logits = self.projection(outputs)\n",
        "        \n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"Sequence-to-sequence model.\"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, src, src_lens, tgt_in):\n",
        "        \"\"\"Teacher forcing forward pass.\"\"\"\n",
        "        # Encode source\n",
        "        _, hidden = self.encoder(src, src_lens)\n",
        "        \n",
        "        # Decode with teacher forcing\n",
        "        logits, _ = self.decoder(tgt_in, hidden)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "        \"\"\"Greedy decoding for inference.\"\"\"\n",
        "        batch_size = src.size(0)\n",
        "        \n",
        "        # Encode source\n",
        "        _, hidden = self.encoder(src, src_lens)\n",
        "        \n",
        "        # Initialize with SOS token\n",
        "        inputs = torch.full(\n",
        "            (batch_size, 1),\n",
        "            sos_id,\n",
        "            dtype=torch.long,\n",
        "            device=src.device\n",
        "        )\n",
        "        \n",
        "        outputs = []\n",
        "        \n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_len):\n",
        "            # Decode one step\n",
        "            logits, hidden = self.decoder(inputs[:, -1:].contiguous(), hidden)\n",
        "            \n",
        "            # Greedy selection\n",
        "            next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "            outputs.append(next_token)\n",
        "            \n",
        "            # Append to input for next step\n",
        "            inputs = torch.cat([inputs, next_token], dim=1)\n",
        "        \n",
        "        # Concatenate all outputs\n",
        "        sequences = torch.cat(outputs, dim=1)\n",
        "        \n",
        "        # Truncate at EOS for each sequence\n",
        "        for i in range(batch_size):\n",
        "            row = sequences[i]\n",
        "            if (row == eos_id).any():\n",
        "                eos_idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
        "                row[eos_idx + 1:] = eos_id\n",
        "        \n",
        "        return sequences\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "encoder = Encoder(\n",
        "    len(src_stoi),\n",
        "    emb_dim,\n",
        "    hid_dim,\n",
        "    num_layers=layers,\n",
        "    dropout=dropout\n",
        ")\n",
        "decoder = Decoder(\n",
        "    len(tgt_stoi),\n",
        "    emb_dim,\n",
        "    hid_dim,\n",
        "    num_layers=layers,\n",
        "    dropout=dropout\n",
        ")\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model parameters: {num_params:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    for src, src_lens, tgt_in, tgt_out, tgt_lens in train_loader:\n",
        "        # Move to device\n",
        "        src = src.to(device)\n",
        "        src_lens = src_lens.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, src_lens, tgt_in)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt_out.reshape(-1)\n",
        "        )\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate statistics\n",
        "        total_loss += float(loss.item())\n",
        "        total_tokens += int((tgt_out != pad_id_tgt).sum().item())\n",
        "    \n",
        "    # Compute perplexities\n",
        "    train_ppl = compute_perplexity(total_loss, total_tokens)\n",
        "    \n",
        "    val_loss, val_tokens = evaluate_nll(val_loader, model, pad_id_tgt, device)\n",
        "    val_ppl = compute_perplexity(val_loss, val_tokens)\n",
        "    \n",
        "    print(f'Epoch {epoch:02d} | train ppl: {train_ppl:.2f} | val ppl: {val_ppl:.2f}')\n",
        "\n",
        "# Save checkpoint\n",
        "checkpoint = {\n",
        "    'model_state': model.state_dict(),\n",
        "    'optimizer_state': optimizer.state_dict(),\n",
        "    'epoch': epochs,\n",
        "    'src_stoi': src_stoi,\n",
        "    'tgt_stoi': tgt_stoi,\n",
        "    'model_cfg': {\n",
        "        'emb': emb_dim,\n",
        "        'hid': hid_dim,\n",
        "        'layers': layers,\n",
        "        'dropout': dropout\n",
        "    }\n",
        "}\n",
        "\n",
        "checkpoint_path = os.path.join(save_dir, 'checkpoint_last.pt')\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print('Saved checkpoint:', checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate: Perplexity and BLEU (Public Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation set\n",
        "val_loss, val_tokens = evaluate_nll(val_loader, model, pad_id_tgt, device)\n",
        "val_ppl = compute_perplexity(val_loss, val_tokens)\n",
        "\n",
        "# Public test set\n",
        "test_loss, test_tokens = evaluate_nll(test_loader, model, pad_id_tgt, device)\n",
        "test_ppl = compute_perplexity(test_loss, test_tokens)\n",
        "\n",
        "# BLEU score\n",
        "bleu = evaluate_bleu(\n",
        "    test_loader,\n",
        "    model,\n",
        "    tgt_itos,\n",
        "    sos_id=sos_id,\n",
        "    eos_id=eos_id,\n",
        "    device=device,\n",
        "    max_len=max_decode_len\n",
        ")\n",
        "\n",
        "print(f'Validation perplexity: {val_ppl:.2f}')\n",
        "print(f'Public test perplexity: {test_ppl:.2f}')\n",
        "print(f'Public test BLEU:       {bleu*100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Private Test (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(private_test_path):\n",
        "    prv_pairs = read_split(private_test_path)\n",
        "    prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n",
        "    prv_loader = DataLoader(\n",
        "        prv_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate,\n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    prv_loss, prv_tokens = evaluate_nll(prv_loader, model, pad_id_tgt, device)\n",
        "    prv_ppl = compute_perplexity(prv_loss, prv_tokens)\n",
        "    \n",
        "    prv_bleu = evaluate_bleu(\n",
        "        prv_loader,\n",
        "        model,\n",
        "        tgt_itos,\n",
        "        sos_id=sos_id,\n",
        "        eos_id=eos_id,\n",
        "        device=device,\n",
        "        max_len=max_decode_len\n",
        "    )\n",
        "    \n",
        "    print(f'Private test perplexity: {prv_ppl:.2f}')\n",
        "    print(f'Private test BLEU:       {prv_bleu*100:.2f}')\n",
        "else:\n",
        "    print('Private test split not found at', private_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Predictions for ML‑Arena (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def decode_to_lines(\n",
        "    loader: DataLoader,\n",
        "    model: nn.Module,\n",
        "    tgt_itos: List[str],\n",
        "    sos_id: int,\n",
        "    eos_id: int,\n",
        "    device: torch.device,\n",
        "    max_len: int\n",
        ") -> List[str]:\n",
        "    \"\"\"Decode all batches to strings.\"\"\"\n",
        "    lines: List[str] = []\n",
        "    \n",
        "    for src, src_lens, tgt_in, tgt_out, tgt_lens in loader:\n",
        "        src = src.to(device)\n",
        "        src_lens = src_lens.to(device)\n",
        "        \n",
        "        predictions = model.greedy_decode(\n",
        "            src, src_lens,\n",
        "            max_len=max_len,\n",
        "            sos_id=sos_id,\n",
        "            eos_id=eos_id\n",
        "        )\n",
        "        \n",
        "        for b in range(src.size(0)):\n",
        "            hyp_ids = predictions[b].tolist()\n",
        "            \n",
        "            # Truncate at EOS\n",
        "            if eos_id in hyp_ids:\n",
        "                hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
        "            \n",
        "            # Convert to tokens, filter padding and SOS\n",
        "            tokens = [\n",
        "                tgt_itos[i] for i in hyp_ids\n",
        "                if i != 0 and i != sos_id\n",
        "            ]\n",
        "            \n",
        "            lines.append(' '.join(tokens))\n",
        "    \n",
        "    return lines\n",
        "\n",
        "\n",
        "# Configuration\n",
        "export_split = 'private'  # 'public' or 'private'\n",
        "export_format = 'tsv'     # 'tsv' or 'jsonl'\n",
        "export_out = 'submissions/private_predictions.tsv'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\n",
        "\n",
        "# Load appropriate split\n",
        "if export_split == 'public':\n",
        "    pairs = read_split(public_test_path)\n",
        "else:\n",
        "    pairs = read_split(private_test_path)\n",
        "\n",
        "exp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi)\n",
        "exp_loader = DataLoader(\n",
        "    exp_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Generate predictions\n",
        "predictions = decode_to_lines(\n",
        "    exp_loader,\n",
        "    model,\n",
        "    tgt_itos,\n",
        "    sos_id=sos_id,\n",
        "    eos_id=eos_id,\n",
        "    device=device,\n",
        "    max_len=max_decode_len\n",
        ")\n",
        "\n",
        "# Export predictions\n",
        "if export_format == 'tsv':\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i, hypothesis in enumerate(predictions):\n",
        "            f.write(f'{i}\\t{hypothesis}\\n')\n",
        "elif export_format == 'jsonl':\n",
        "    import json\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i, hypothesis in enumerate(predictions):\n",
        "            json_obj = {'id': i, 'hyp': hypothesis}\n",
        "            f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f'Wrote {len(predictions)} predictions to {export_out}')\n",
        "print('Adjust if ML‑Arena requires a different schema.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
